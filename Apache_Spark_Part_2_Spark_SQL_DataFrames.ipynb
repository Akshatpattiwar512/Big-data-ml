{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Spark-Part 2: Spark SQL/DataFrames",
      "provenance": [],
      "authorship_tag": "ABX9TyPeBREVQJ9pwE+UHA2g5/mi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshatpattiwar512/Big-data-ml/blob/main/Apache_Spark_Part_2_Spark_SQL_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNnsBzo__XLs"
      },
      "source": [
        "#Apache Spark-Part 2: Spark SQL/DataFrames\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06vcn00EREx"
      },
      "source": [
        "Following notebook is based on [Apache Spark-Part 2: Spark SQL/DataFrames](https://www.linkedin.com/pulse/apache-spark-part-2-spark-sqldataframes-akshat-pattiwar/) article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocJKOk9D_pXw"
      },
      "source": [
        "DataFrames are logically identical to relational tables or DataFrames in Python/R, but they have a lot of optimizations hidden behind the scenes. We can make DataFrames from collections, HIVE tables, Relational tables, and RDDs in a variety of methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHT4p8G2AG41"
      },
      "source": [
        "##Installing pySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fza2vvK_--6",
        "outputId": "9ba40c38-b5e3-4d49-fbe6-a170f2f49d56"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 16.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=1c4be779a8993829c11895bd8b5d47191fc0786a1f0d4fb29eaea6592fb92c6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi26Y44kALxa"
      },
      "source": [
        "##Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUMWTX144eAs"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPvOpgWQARpi"
      },
      "source": [
        "##Create SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88-m6KjA41OX"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RS0vK2rAcvl"
      },
      "source": [
        "##Create DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5BKwuQv41d0"
      },
      "source": [
        "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
        "    (2, \"BBB\", \"dept1\", 1100),\n",
        "    (3, \"CCC\", \"dept1\", 3000),\n",
        "    (4, \"DDD\", \"dept1\", 1500),\n",
        "    (5, \"EEE\", \"dept2\", 8000),\n",
        "    (6, \"FFF\", \"dept2\", 7200),\n",
        "    (7, \"GGG\", \"dept3\", 7100),\n",
        "    (8, \"HHH\", \"dept3\", 3700),\n",
        "    (9, \"III\", \"dept3\", 4500),\n",
        "    (10, \"JJJ\", \"dept5\", 3400)]\n",
        "\n",
        "dept = [(\"dept1\", \"Department - 1\"),\n",
        "        (\"dept2\", \"Department - 2\"),\n",
        "        (\"dept3\", \"Department - 3\"),\n",
        "        (\"dept4\", \"Department - 4\")\n",
        "\n",
        "       ]\n",
        "\n",
        "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
        "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7QRElG3AhEy"
      },
      "source": [
        "##Display DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyjF7Spe41j7",
        "outputId": "e134966a-78cc-4789-b9c6-9cc25710f1df"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  8| HHH|dept3|  3700|\n",
            "|  9| III|dept3|  4500|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7mwLHKrAnw6"
      },
      "source": [
        "##Basic operations on DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv3Ccj8dAqYC"
      },
      "source": [
        "###count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UolzuP22Asba"
      },
      "source": [
        "Count the number of rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6SbjRVqAijJ",
        "outputId": "6516a4fd-fbc9-4160-df95-25e606dd58e5"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc4TUbaNAyM7"
      },
      "source": [
        "###columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmE7Z1FeA01C"
      },
      "source": [
        "Access the names of columns in the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W10ky5ZZ41o6",
        "outputId": "198d3fc3-d514-4926-cda9-b23d730dacab"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'name', 'dept', 'salary']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hid-VO6HA5HI"
      },
      "source": [
        "###dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B6YtLy2A_PT"
      },
      "source": [
        "Access the DataType of columns within the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRk-i0sq41t2",
        "outputId": "4d3f0938-62c3-4c29-88b1-645b7250d5c5"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'bigint'),\n",
              " ('name', 'string'),\n",
              " ('dept', 'string'),\n",
              " ('salary', 'bigint')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqFMac21BCdN"
      },
      "source": [
        "###schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elUlAFldBEMF"
      },
      "source": [
        "Check how Spark stores the schema of the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCUS30by41xw",
        "outputId": "7ccf7657-a73b-4eb1-96ed-2c59748466fb"
      },
      "source": [
        "df.schema"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(id,LongType,true),StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,LongType,true)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eCJH2ekBIz7"
      },
      "source": [
        "###printSchema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s1LE1wI412M",
        "outputId": "65f30997-dcd2-4b11-d00e-7715cd841d62"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUpleezBNDT"
      },
      "source": [
        "###select"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-RjIjoVBQF7"
      },
      "source": [
        "Select particular columns from the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZoZtPF6416j",
        "outputId": "cb7f2289-2292-4791-d74c-06bd33f5255f"
      },
      "source": [
        "df.select(\"id\", \"name\").show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| AAA|\n",
            "|  2| BBB|\n",
            "|  3| CCC|\n",
            "|  4| DDD|\n",
            "|  5| EEE|\n",
            "|  6| FFF|\n",
            "|  7| GGG|\n",
            "|  8| HHH|\n",
            "|  9| III|\n",
            "| 10| JJJ|\n",
            "+---+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1NcLSDaBSt7"
      },
      "source": [
        "###filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8xprWPxBWUt"
      },
      "source": [
        "* Filter the rows based on some condition.\n",
        "\n",
        "* Let's try to find the rows with id = 1.\n",
        "\n",
        "* There are different ways to specify the condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeCHCi5cBRlb",
        "outputId": "a6629057-2695-4016-eca4-afba9f9cb13c"
      },
      "source": [
        "# Execute below statement by uncommeting them.\n",
        "\n",
        "df.filter(df[\"id\"] == 1).show()\n",
        "#df.filter(df.id == 1).show()\n",
        "#df.filter(col(\"id\") == 1).show()\n",
        "#df.filter(\"id = 1\").show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQy64exqBlrz"
      },
      "source": [
        "###drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIYwGQH9Bnpe"
      },
      "source": [
        "Drop a particular Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDlYHnOOBh9Q",
        "outputId": "45ceb274-1263-44c0-f4c8-15865e571851"
      },
      "source": [
        "newdf = df.drop(\"id\")\n",
        "newdf.show(2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+------+\n",
            "|name| dept|salary|\n",
            "+----+-----+------+\n",
            "| AAA|dept1|  1000|\n",
            "| BBB|dept1|  1100|\n",
            "+----+-----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yptlhTzKBwkl"
      },
      "source": [
        "###Aggregations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55w8wCmFB0LL"
      },
      "source": [
        "We can use the groupBy function to group the data and then use the \"agg\" function to perform aggregation on grouped data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5OS549HBpJ1",
        "outputId": "7b9af173-fc18-40d5-9662-43733ef7ed5d"
      },
      "source": [
        "(df.groupBy(\"dept\")\n",
        "    .agg(\n",
        "        count(\"salary\").alias(\"count\"),\n",
        "        sum(\"salary\").alias(\"sum\"),\n",
        "        max(\"salary\").alias(\"max\"),\n",
        "        min(\"salary\").alias(\"min\"),\n",
        "        avg(\"salary\").alias(\"avg\")\n",
        "        ).show()\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+-----+----+----+------+\n",
            "| dept|count|  sum| max| min|   avg|\n",
            "+-----+-----+-----+----+----+------+\n",
            "|dept5|    1| 3400|3400|3400|3400.0|\n",
            "|dept3|    3|15300|7100|3700|5100.0|\n",
            "|dept1|    4| 6600|3000|1000|1650.0|\n",
            "|dept2|    2|15200|8000|7200|7600.0|\n",
            "+-----+-----+-----+----+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OibYyBwPB56U"
      },
      "source": [
        "###Sorting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUw0Uzi9B8Dl"
      },
      "source": [
        "Sort the data based on \"salary\". By default, sorting will be done in Ascending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn3xvX76B2CK",
        "outputId": "af56fbd2-7575-4a30-8a2d-29fda0d110aa"
      },
      "source": [
        "df.sort(\"salary\").show(5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  3| CCC|dept1|  3000|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt4Ao8SfB9vl",
        "outputId": "f2648c51-4b7f-4077-915a-200221dee88a"
      },
      "source": [
        "# Sort the data in descending order.\n",
        "df.sort(desc(\"salary\")).show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  9| III|dept3|  4500|\n",
            "|  8| HHH|dept3|  3700|\n",
            "+---+----+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq2bTtAbCCXi"
      },
      "source": [
        "###Derived Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM9ZTbaoCECj"
      },
      "source": [
        "We can use the \"withColumn\" function to derive the column based on existing columns…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic39JnpFB_NF",
        "outputId": "bf77257d-6b09-44d9-b879-b9e1284ef328"
      },
      "source": [
        "df.withColumn(\"bonus\", col(\"salary\") * .1).show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+-----+\n",
            "| id|name| dept|salary|bonus|\n",
            "+---+----+-----+------+-----+\n",
            "|  1| AAA|dept1|  1000|100.0|\n",
            "|  2| BBB|dept1|  1100|110.0|\n",
            "|  3| CCC|dept1|  3000|300.0|\n",
            "|  4| DDD|dept1|  1500|150.0|\n",
            "|  5| EEE|dept2|  8000|800.0|\n",
            "|  6| FFF|dept2|  7200|720.0|\n",
            "|  7| GGG|dept3|  7100|710.0|\n",
            "|  8| HHH|dept3|  3700|370.0|\n",
            "|  9| III|dept3|  4500|450.0|\n",
            "| 10| JJJ|dept5|  3400|340.0|\n",
            "+---+----+-----+------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4UM8o7ACHgd"
      },
      "source": [
        "###Joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s38dDBGECKCS"
      },
      "source": [
        "perform various types of joins on multiple DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnO1ztHrCFrh",
        "outputId": "5e99b0bb-6cf0-42de-907e-8ea11509f1b0"
      },
      "source": [
        "# Inner JOIN.\n",
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+-----+--------------+\n",
            "| id|name| dept|salary|   id|          name|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|  9| III|dept3|  4500|dept3|Department - 3|\n",
            "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNc7HVLqCNm7"
      },
      "source": [
        "###Left Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bik5CWAlCLf6",
        "outputId": "83ba8e1b-724d-4017-dd92-3f3fdc3a4767"
      },
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+-----+--------------+\n",
            "| id|name| dept|salary|   id|          name|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "| 10| JJJ|dept5|  3400| null|          null|\n",
            "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|  9| III|dept3|  4500|dept3|Department - 3|\n",
            "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSpPxxWlCRkF"
      },
      "source": [
        "###Right Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB5w_QXnCPlz",
        "outputId": "27433696-c138-41dc-ee68-acfe15a0182a"
      },
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+-----+--------------+\n",
            "|  id|name| dept|salary|   id|          name|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|   9| III|dept3|  4500|dept3|Department - 3|\n",
            "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|null|null| null|  null|dept4|Department - 4|\n",
            "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHx8sWLwCU9S"
      },
      "source": [
        "###Full Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGy-SBAxCTUJ",
        "outputId": "d53d9174-83ae-4dde-d925-dceefb8fd42a"
      },
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+-----+--------------+\n",
            "|  id|name| dept|salary|   id|          name|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "|  10| JJJ|dept5|  3400| null|          null|\n",
            "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|   9| III|dept3|  4500|dept3|Department - 3|\n",
            "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|null|null| null|  null|dept4|Department - 4|\n",
            "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PndEb7ZCY-C"
      },
      "source": [
        "###SQL Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U37fDLcKCbCy"
      },
      "source": [
        "* Executing SQL like queries.\n",
        "\n",
        "* We can perform data analysis by writing SQL like queries as well. In order to perform the SQL like queries, we need to register the DataFrame as a Temporary View."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSLDv0NGCXbu",
        "outputId": "da20bf69-9355-4fc6-d9d9-694fd3940270"
      },
      "source": [
        "# Register DataFrame as Temporary Table\n",
        "df.createOrReplaceTempView(\"temp_table\")\n",
        "\n",
        "# Execute SQL-Like query.\n",
        "spark.sql(\"select * from temp_table where id = 1\").show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OquN4RMICko6"
      },
      "source": [
        "###Reading HIVE table as DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e0pk8hRCjXh"
      },
      "source": [
        "# DB_NAME : Name of the the HIVE Database\n",
        "# TBL_NAME : Name of the HIVE Table\n",
        "\n",
        "# Uncomment below statement and provide your Hive DB and Table details.\n",
        "#df = spark.table(\"DB_NAME\".\"TBL_NAME\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfD6bFzeDV99"
      },
      "source": [
        "###Save DataFrame as HIVE Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx7mCQeKCplL"
      },
      "source": [
        "# Uncomment below statement and provide your Hive DB and Table details.\n",
        "#df.write.saveAsTable(\"DB_NAME.TBL_NAME\")\n",
        "\n",
        "# We can also select the \"mode\" argument for \"overwrite\", \"append\", \"error\" etc.\n",
        "# df.write.saveAsTable(\"DB_NAME.TBL_NAME\", mode=\"overwrite\")\n",
        "\n",
        "# Note: By default, the operation will save the DataFrame as a HIVE Managed/Internal table"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gof06ZK_DZY0"
      },
      "source": [
        "###Saving the DataFrame as a HIVE External table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhO0LptyDX-y"
      },
      "source": [
        "# Uncomment below statement and provide your Hive DB and Table details.\n",
        "# Also, provide the path where you would like to save the Data.\n",
        "\n",
        "# df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQmcu1puDdak"
      },
      "source": [
        "###Create a DataFrame from CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9bPIHSeDfn7"
      },
      "source": [
        "We can create a DataFrame using a CSV file and can specify various options like a separator, header, schema, inferSchema, and various other options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDvSAk82Dbrb"
      },
      "source": [
        "# Un-comment below code and provide necessary details.\n",
        "# df = spark.read.csv(\"path_to_csv_file\", sep=\"|\", header=True, inferSchema=True)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJkNxnqDiuz"
      },
      "source": [
        "###Save a DataFrame as a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjyRwDp2DhO1"
      },
      "source": [
        "# Un-comment below code and provide necessary details.\n",
        "# df.write.csv(\"path_to_CSV_File\", sep=\"|\", header=True, mode=\"overwrite\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ITZ3SoaDl4h"
      },
      "source": [
        "###Create a DataFrame from a relational table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDn2VHEkDnwS"
      },
      "source": [
        "We can read the data from relational databases using a JDBC URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRp2Zpt6DkvF"
      },
      "source": [
        "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
        "# TBL_NAME : Name of the relational table.\n",
        "# USER_NAME : user name to connect to DataBase.\n",
        "# PASSWORD: password to connect to DataBase.\n",
        "\n",
        "# Un-comment below code and provide necessary details.\n",
        "# relational_df = spark.read.format('jdbc')\n",
        "#                        .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
        "#                        .load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nk5-_ZWDrlt"
      },
      "source": [
        "###Save the DataFrame as a relational table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdrZRO_XDtQ1"
      },
      "source": [
        "We can save the DataFrame as a relational table using a JDBC URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxjL2yjCDt-S"
      },
      "source": [
        "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
        "# TBL_NAME : Name of the relational table.\n",
        "# USER_NAME : user name to connect to DataBase.\n",
        "# PASSWORD: password to connect to DataBase.\n",
        "\n",
        "# Un-comment below code and provide necessary details.\n",
        "# relational_df.write.format('jdbc')\n",
        "#                    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
        "#                    .mode('overwrite')\n",
        "#                    .save()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqtqv65FD128"
      },
      "source": [
        "--------------------------------------------------------------------------------------------------------------------------------------\n",
        "##**Part 2 Ends**\n",
        "--------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}