{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Spark-Part 3: Advance Dataframe Operations.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgCPmFZ453I9qYjKnsw8j/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshatpattiwar512/Big-data-ml/blob/main/Apache_Spark_Part_3_Advance_Dataframe_Operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uGBGpthJtAe"
      },
      "source": [
        "#Apache Spark-Part 3: Advance Dataframe Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2p8CMUJ18h"
      },
      "source": [
        "Following notebook is based on [Apache Spark-Part 3: Advance Dataframe Operations](https://www.linkedin.com/pulse/apache-spark-part-3-advance-dataframe-operations-akshat-pattiwar/) article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK2KsIuSKHRK"
      },
      "source": [
        "This notebook has implementation of Advanced dataframe operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wisoz_RKdrY"
      },
      "source": [
        "##Installing pySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcsMTE0RFtiJ",
        "outputId": "80b27095-0919-40aa-8189-6f233e6f830f"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 73kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=4141b67b8811fea8354684e44d42dff47f38e6780bbc1eb053996395fdb14001\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vmsLkOIKmAo"
      },
      "source": [
        "##Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaUS4fi6Kjp_"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import broadcast\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaViBf_mKt5J"
      },
      "source": [
        "##Create SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V39G58NyKrFP"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QFFGD8qK0rr"
      },
      "source": [
        "##Create DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS6Atl2ZKxBF"
      },
      "source": [
        "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
        "    (2, \"BBB\", \"dept1\", 1100),\n",
        "    (3, \"CCC\", \"dept1\", 3000),\n",
        "    (4, \"DDD\", \"dept1\", 1500),\n",
        "    (5, \"EEE\", \"dept2\", 8000),\n",
        "    (6, \"FFF\", \"dept2\", 7200),\n",
        "    (7, \"GGG\", \"dept3\", 7100),\n",
        "    (None, None, None, 7500),\n",
        "    (9, \"III\", None, 4500),\n",
        "    (10, None, \"dept5\", 2500)]\n",
        "\n",
        "dept = [(\"dept1\", \"Department - 1\"),\n",
        "        (\"dept2\", \"Department - 2\"),\n",
        "        (\"dept3\", \"Department - 3\"),\n",
        "        (\"dept4\", \"Department - 4\")\n",
        "       ]\n",
        "\n",
        "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
        "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
        "\n",
        "# Create Temp Tables\n",
        "df.createOrReplaceTempView(\"empdf\")\n",
        "deptdf.createOrReplaceTempView(\"deptdf\")\n",
        "\n",
        "# Save as HIVE tables.\n",
        "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
        "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pyEFJNjK6Eo"
      },
      "source": [
        "##BroadCast Join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz87Q_ZvK-O6"
      },
      "source": [
        "The size of the broadcast table is 10 MB. However, we can change the threshold upto 8GB as per the official documentation of Spark 2.3.\n",
        "\n",
        "* We can check the size of the broadcast table as follow :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1jO3tPdK4GA",
        "outputId": "57f8cf6b-10a4-4c9e-eddc-bb14b295a252"
      },
      "source": [
        "size = int(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")) / (2048)\n",
        "print(\"Default size of broadcast table is {0} MB.\".format(size))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default size of broadcast table is 25600.0 MB.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dT5-dDqLCMs"
      },
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fFWRWanLaxo"
      },
      "source": [
        "##Caching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O67FObEJLckJ"
      },
      "source": [
        "We can use cache/persist function to keep the dataframe in-memory. It may improve the performance of your spark application significantly if we cache the data which we need to use very frequently in our application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpWlDfPcLKVu",
        "outputId": "ae028820-c243-4cf0-b298-de3b6ca8ae85"
      },
      "source": [
        "df.cache()\n",
        "df.count()\n",
        "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
        "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory Used : True\n",
            "Disk Used : True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVK-KusmLgyQ"
      },
      "source": [
        "When we use cache function it will use storage level as Memory_Only until Spark 2.0.2. Since Spark 2.1.x it is Memory_and_DISK.\n",
        "\n",
        "However, if we need to specify the various available storage levels we can use persist method. For example, if we need to keep the data in Memory only we can use below snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPSWvRrkLeVY"
      },
      "source": [
        "from pyspark.storagelevel import StorageLevel"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZpPAMm3LjOK",
        "outputId": "4c9498c0-711c-4a52-bccb-5b6a06febbb2"
      },
      "source": [
        "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
        "deptdf.count()\n",
        "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
        "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory Used : True\n",
            "Disk Used : True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vliDg5HFLmQo"
      },
      "source": [
        "##Un-Persist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iHSpMIULnnp"
      },
      "source": [
        "It is also important to un-cache the data when it will no longer be required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhao1y2ILk__",
        "outputId": "178c5276-0a60-4474-f919-ec1f5b751ce1"
      },
      "source": [
        "df.unpersist()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_74A-pW4LtdQ"
      },
      "source": [
        "##SQL Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYrOAX1LLvjo"
      },
      "source": [
        "We can use SQL expression for the manipulation of data as well. We have \"expr\" function and also a variant of a select method as \"selectExpr\" for evaluation of SQL expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbC6ChhzLpFQ",
        "outputId": "4824db89-9e2d-4287-a434-13f23afa1711"
      },
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Let's try to categorige the salary under Low, Mid and High as per below categorization.\n",
        "\n",
        "# 0-2000 : low_salary\n",
        "# 2001 - 5000 : mid_salary\n",
        "# > 5001 : high_salary\n",
        "\n",
        "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
        "               else case when salary > 2000 then 'mid_salary'\n",
        "                    else case when salary > 0 then 'low_salary'\n",
        "                         else 'invalid_salary'\n",
        "                              end\n",
        "                         end\n",
        "                end as salary_level\"\"\"\n",
        "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
        "newdf.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTujJizhL2tA"
      },
      "source": [
        "##Using selectExpr function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyVjYYpILzmm",
        "outputId": "0f29706d-3cd5-4252-a916-b58b472d28bf"
      },
      "source": [
        "newdf = df.selectExpr(\"*\", cond)\n",
        "newdf.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiFNBSKPL6fU"
      },
      "source": [
        "##User Defined Functions (UDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk-Sz4kpL8H7"
      },
      "source": [
        "Often we need to write the function based on the our very specific requirement. Here we can leverage the udfs. We can write our own functions in language like python and register the function as udf, then we can use the function for DataFrame operations.\n",
        "\n",
        "* Python function to find the salary_level for a given salary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3SsYBd3L40g"
      },
      "source": [
        "def detSalary_Level(sal):\n",
        "    level = None\n",
        "\n",
        "    if(sal > 5000):\n",
        "        level = 'high_salary'\n",
        "    elif(sal > 2000):\n",
        "        level = 'mid_salary'\n",
        "    elif(sal > 0):\n",
        "        level = 'low_salary'\n",
        "    else:\n",
        "        level = 'invalid_salary'\n",
        "    return level"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cke0dAMRMBGR"
      },
      "source": [
        "* Then register the function \"detSalary_Level\" as UDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDO-qvv5L-6X"
      },
      "source": [
        "sal_level = udf(detSalary_Level, StringType())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2towX9koMEXt"
      },
      "source": [
        "* Apply function to determine the salary_level for given salary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoJHM1s1MDf_",
        "outputId": "c30681ba-3184-41a7-b18a-e2f1e0f3c3ab"
      },
      "source": [
        "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
        "newdf.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebKBSp2TMIOh"
      },
      "source": [
        "##Working with NULL Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S1IlWFYMKCI"
      },
      "source": [
        "NULL values are always tricky to deal with irrespective of the Framework or language we use. Here in spark we have few specific function to deal with NULL values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M8JpVzmML4o"
      },
      "source": [
        "###isNull()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZR-JThjMQBA"
      },
      "source": [
        "This function will help us to find the null values for any given columns. For e.g. if we need to find the columns where id columns contains the null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkPQe7dbMGdU",
        "outputId": "c49e03f6-75c2-469a-a377-575ad14337de"
      },
      "source": [
        "newdf = df.filter(df[\"dept\"].isNull())\n",
        "newdf.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+----+------+\n",
            "|  id|name|dept|salary|\n",
            "+----+----+----+------+\n",
            "|null|null|null|  7500|\n",
            "|   9| III|null|  4500|\n",
            "+----+----+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyEaTNN2MSw3"
      },
      "source": [
        "###isNotNull()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RrJ0HnMUDg"
      },
      "source": [
        "This function works opposite to isNull() function and will return all the not null values for a particular function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqvL054JMRai",
        "outputId": "ad4b4127-581e-4ecd-c1ae-f4f8cf575042"
      },
      "source": [
        "newdf = df.filter(df[\"dept\"].isNotNull())\n",
        "newdf.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "| 10|null|dept5|  2500|\n",
            "+---+----+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hht2UYPMWnK"
      },
      "source": [
        "###fillna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGKLqcPuMYWY"
      },
      "source": [
        "This function will help us to replace Null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-GyOaNkMVUq",
        "outputId": "bf459fda-2dc2-4e90-8577-098db28cbb64"
      },
      "source": [
        "# Replace -1 where the salary is null.\n",
        "newdf = df.fillna(\"INVALID\", [\"dept\"])\n",
        "newdf.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+-------+------+\n",
            "|  id|name|   dept|salary|\n",
            "+----+----+-------+------+\n",
            "|   1| AAA|  dept1|  1000|\n",
            "|   2| BBB|  dept1|  1100|\n",
            "|   3| CCC|  dept1|  3000|\n",
            "|   4| DDD|  dept1|  1500|\n",
            "|   5| EEE|  dept2|  8000|\n",
            "|   6| FFF|  dept2|  7200|\n",
            "|   7| GGG|  dept3|  7100|\n",
            "|null|null|INVALID|  7500|\n",
            "|   9| III|INVALID|  4500|\n",
            "|  10|null|  dept5|  2500|\n",
            "+----+----+-------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RspaFl52MbmZ"
      },
      "source": [
        "###dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKV0j8JCMe6y"
      },
      "source": [
        "This function will help us to remove the rows with null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDlGajnNMZl3",
        "outputId": "a4daeb0b-95ef-4c7d-b465-e60a79bcfb39"
      },
      "source": [
        "# Remove all rows which contains any null values.\n",
        "newdf = df.dropna()\n",
        "newdf.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "+---+----+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTroNiKyMg3i",
        "outputId": "c2e91cad-8024-4f50-991f-6b2b5aa35183"
      },
      "source": [
        "# Remove all rows which contains all null values.\n",
        "newdf = df.dropna(how = \"all\")\n",
        "newdf.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+\n",
            "|  id|name| dept|salary|\n",
            "+----+----+-----+------+\n",
            "|   1| AAA|dept1|  1000|\n",
            "|   2| BBB|dept1|  1100|\n",
            "|   3| CCC|dept1|  3000|\n",
            "|   4| DDD|dept1|  1500|\n",
            "|   5| EEE|dept2|  8000|\n",
            "|   6| FFF|dept2|  7200|\n",
            "|   7| GGG|dept3|  7100|\n",
            "|null|null| null|  7500|\n",
            "|   9| III| null|  4500|\n",
            "|  10|null|dept5|  2500|\n",
            "+----+----+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqEvZGY5M145",
        "outputId": "491eefb7-e36e-4e6b-b037-e79a9adf0f4e"
      },
      "source": [
        "# Remove all rows where columns : dept is null.\n",
        "newdf = df.dropna(subset = \"dept\")\n",
        "newdf.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "| 10|null|dept5|  2500|\n",
            "+---+----+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mvIrtmINJYq"
      },
      "source": [
        "##Partitioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcJHIfuxNLEA"
      },
      "source": [
        "Partitioning is very important aspect to control the parallelism for spark Application.\n",
        "\n",
        "* Check number of partitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f8tx5tpNC2X",
        "outputId": "b3215b18-743f-44e1-a4b2-10a57e2940cf"
      },
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bweRsQTNiwp"
      },
      "source": [
        "* Increase number of partitions. For e.g. Increase partitions to 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByijHXovNQJT",
        "outputId": "81c3b32a-931d-414b-e94c-36dc08df64ff"
      },
      "source": [
        "newdf = df.repartition(6)\n",
        "newdf.rdd.getNumPartitions()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnX18Ar4NyWa"
      },
      "source": [
        "**Note : This is expensive operations since it require shuffling of data across the workers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxKh71wtN5WZ"
      },
      "source": [
        "* Decrease Number of Partitions. For e.g. decrease partitions to 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4skcszDNlIC",
        "outputId": "9ec201a9-56d1-49c1-f3a8-c1b27379e868"
      },
      "source": [
        "newdf = df.coalesce(2)\n",
        "newdf.rdd.getNumPartitions()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHZCOoeOAxP"
      },
      "source": [
        "* By default the number of partitions for Spark SQL is 200.\n",
        "\n",
        "* But we can eet the Number of partitions at Spark Application level as well. For e.g. set to 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKtBB5bjN_XG",
        "outputId": "3a528345-5a54-4c7b-ddcd-5143a72fce70"
      },
      "source": [
        "# Set number of partitions as Spark Application.\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
        "\n",
        "# Check the number of patitions.\n",
        "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
        "print(\"No of Partitions : {0}\".format(num_part))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of Partitions : 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGFOm9erOIjA"
      },
      "source": [
        "##Catalog API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BILrD7XOKzP"
      },
      "source": [
        "Spark Catalog is a user-facing API, which you can access using SparkSession.catalog."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_DQeP9YOMmP"
      },
      "source": [
        "* listDatabases()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJVzQIQBOOtE"
      },
      "source": [
        "It will return all the databases along with their Location on file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhPCw_5nOHHz",
        "outputId": "8c4a9851-e119-49c3-d0bc-c3df5f2d445a"
      },
      "source": [
        "spark.catalog.listDatabases()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='default database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xzVVktAORxA"
      },
      "source": [
        "* listTables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLBAmDUwOTZA"
      },
      "source": [
        "It will return all the tables for a given database along with the information like Table type (External/Managed) and whether a particular table is temporary or permanent. This includes all temporary views."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOzUc445OQX8",
        "outputId": "3b592425-de37-43e9-b3cf-47806ac37e51"
      },
      "source": [
        "spark.catalog.listTables(\"default\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='hive_deptdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='hive_empdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='deptdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='empdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-KCKPtbOWbP"
      },
      "source": [
        "* listColumns()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6PtoxMOYAY"
      },
      "source": [
        "It will return all the columns for a particular table in DataBase. Also, it will return datatype, if the column is used in Partitioning or Bucketing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ9UmvHaOVON",
        "outputId": "e7843026-5f52-464a-86fe-1abdf4602de8"
      },
      "source": [
        "spark.catalog.listColumns(\"hive_empdf\", \"default\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Column(name='id', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='dept', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='salary', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMppeKigObBf"
      },
      "source": [
        "* listFunctions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL-GRCoKOdWX"
      },
      "source": [
        "It will return all the available function in spark Session along with the information whether it is temporary or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4K7cIJjOZwB",
        "outputId": "64328dd4-342a-40d7-b52f-16c52e76dcff"
      },
      "source": [
        "spark.catalog.listFunctions()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
              " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
              " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
              " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
              " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
              " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
              " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
              " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
              " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
              " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
              " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
              " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
              " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
              " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
              " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
              " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
              " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
              " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
              " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
              " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
              " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
              " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
              " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
              " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
              " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
              " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
              " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
              " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
              " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
              " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
              " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
              " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
              " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
              " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
              " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
              " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
              " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
              " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
              " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
              " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
              " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
              " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
              " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
              " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
              " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
              " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
              " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
              " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
              " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
              " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
              " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
              " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
              " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
              " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
              " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
              " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
              " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
              " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
              " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
              " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
              " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
              " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
              " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
              " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
              " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
              " Function(name='cube', description=None, className='org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True),\n",
              " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
              " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
              " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
              " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
              " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
              " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
              " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
              " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
              " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
              " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePart', isTemporary=True),\n",
              " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
              " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
              " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
              " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
              " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
              " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
              " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
              " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
              " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
              " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
              " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
              " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
              " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
              " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
              " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
              " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
              " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
              " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
              " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
              " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
              " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
              " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
              " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
              " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
              " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
              " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
              " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
              " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
              " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
              " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
              " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
              " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
              " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
              " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
              " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
              " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
              " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
              " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
              " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
              " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
              " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
              " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
              " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
              " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
              " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
              " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
              " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
              " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
              " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
              " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
              " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
              " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
              " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
              " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
              " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
              " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
              " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
              " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
              " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
              " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
              " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
              " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
              " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
              " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
              " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
              " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
              " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
              " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
              " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
              " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
              " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
              " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
              " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
              " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
              " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
              " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
              " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
              " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
              " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
              " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
              " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
              " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
              " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
              " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
              " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
              " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
              " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
              " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
              " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
              " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
              " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
              " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
              " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
              " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
              " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
              " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
              " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
              " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
              " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
              " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
              " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
              " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
              " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
              " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
              " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
              " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
              " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
              " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
              " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
              " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
              " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
              " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
              " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
              " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
              " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
              " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
              " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
              " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='rollup', description=None, className='org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True),\n",
              " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
              " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
              " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
              " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
              " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
              " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
              " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
              " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
              " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
              " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
              " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
              " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
              " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
              " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
              " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
              " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
              " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
              " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
              " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
              " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
              " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
              " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
              " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
              " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
              " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
              " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
              " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
              " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
              " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
              " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
              " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
              " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
              " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
              " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
              " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
              " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
              " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
              " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
              " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
              " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
              " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
              " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
              " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
              " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
              " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
              " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
              " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
              " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
              " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
              " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
              " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
              " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
              " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
              " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
              " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
              " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
              " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
              " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
              " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
              " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
              " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
              " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
              " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
              " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
              " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
              " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
              " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
              " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
              " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
              " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
              " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
              " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AKHF_j-Oiwf"
      },
      "source": [
        "* currentDatabase()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAsyCnkPOkqI"
      },
      "source": [
        "Get the current DataBase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ACz1EXW_OesX",
        "outputId": "31edeab5-402b-4a76-e9f6-823a9ea0f5bc"
      },
      "source": [
        "spark.catalog.currentDatabase()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'default'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzm0nz9iOou1"
      },
      "source": [
        "* setCurrentDatabase()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ws8QOdrOsIn"
      },
      "source": [
        "Set the Current Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdAyybIjOmVL"
      },
      "source": [
        "# Un-comment below code and specify the DB_Name to be set as current Database.\n",
        "#spark.catalog.setCurrentDatabase(<DB_Name>)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkycvIYaOvKo"
      },
      "source": [
        "* cacheTable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QnG_S3jOwo8"
      },
      "source": [
        "cache a particular table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my8A4h2-OuYn"
      },
      "source": [
        "spark.catalog.cacheTable(\"default.hive_empdf\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1q3ShZdOzoX"
      },
      "source": [
        "* isCached()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r6EYOtCO24v"
      },
      "source": [
        "Check if Table is cached or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbY0LuzBOyWl",
        "outputId": "7a386b15-c14b-4567-8acc-5039df950e4e"
      },
      "source": [
        "spark.catalog.isCached(\"default.hive_empdf\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs35mx3qO4-s"
      },
      "source": [
        "* uncacheTable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GAXTstrO6sm"
      },
      "source": [
        "Un-cache particular table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vmm7JWoO3ni"
      },
      "source": [
        "spark.catalog.uncacheTable(\"default.hive_empdf\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8VkGlPgO8Ps",
        "outputId": "90717f1f-2b7d-48e8-d0dc-ea291a83acc7"
      },
      "source": [
        "# Verify uncached table. Now you will see that it will return \"False\" which means table is not cached.\n",
        "spark.catalog.isCached(\"default.hive_empdf\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8Or14nO-5R"
      },
      "source": [
        "* clearCache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV3-6cVfPAmx"
      },
      "source": [
        "Un-cache all table in Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gA27IYzO-MX"
      },
      "source": [
        "spark.catalog.clearCache()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07Z7DdxPE7h"
      },
      "source": [
        "-------------------------------------------------------------------\n",
        "# End of part 3\n",
        "\n",
        "-------------------------------------------------------------------"
      ]
    }
  ]
}